# train_baseline.py
import os, json, argparse, numpy as np, torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib
import matplotlib.pyplot as plt
from tqdm import tqdm

from .utils.seed import set_seed
from .utils.data import PairNPZDataset
from .models.simple_compare_cnn import CompareNet, count_params

def evaluate(model, loader, device, use_aux=False):
    """评估函数，返回准确率、F1和所有预测结果用于混淆矩阵"""
    model.eval()
    ys, ps, probs = [], [], []
    total_loss = 0
    criterion = nn.BCEWithLogitsLoss()
    
    with torch.no_grad():
        for xa, xb, y in loader:
            xa = xa.to(device).float()
            xb = xb.to(device).float()
            y = y.to(device).float()
            
            logit = model(xa, xb)
            if isinstance(logit, tuple):
                logit = logit[0]  # 只取主logit
                
            loss = criterion(logit, y.float())
            total_loss += loss.item()
            
            prob = torch.sigmoid(logit)
            pred = (prob >= 0.5).long()
            
            ys.append(y.long().cpu().numpy())
            ps.append(pred.cpu().numpy())
            probs.append(prob.cpu().numpy())
    
    y_true = np.concatenate(ys)
    y_pred = np.concatenate(ps)
    y_probs = np.concatenate(probs)
    
    # 计算准确率
    acc = (y_true == y_pred).mean().item()
    
    # 计算macro-F1
    f1s = []
    for cls in [0, 1]:
        tp = np.sum((y_true == cls) & (y_pred == cls))
        fp = np.sum((y_true != cls) & (y_pred == cls))
        fn = np.sum((y_true == cls) & (y_pred != cls))
        prec = tp / (tp + fp + 1e-12)
        rec = tp / (tp + fn + 1e-12)
        f1 = 2 * prec * rec / (prec + rec + 1e-12)
        f1s.append(f1)
    f1_macro = float(np.mean(f1s))
    
    avg_loss = total_loss / len(loader)
    
    return acc, f1_macro, avg_loss, y_true, y_pred, y_probs

def maybe_swap_batch(xa, xb, y):
    """交换增强"""
    b = xa.size(0)
    mask = torch.rand(b, device=xa.device) < 0.5
    xa2, xb2, y2 = xa.clone(), xb.clone(), y.clone()
    xa2[mask], xb2[mask] = xb[mask], xa[mask]
    if y2.dtype.is_floating_point:
        y2[mask] = 1.0 - y2[mask]
    else:
        y2[mask] = 1 - y2[mask]
    return xa2, xb2, y2

class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes=2, smoothing=0.1, dim=-1):
        super(LabelSmoothingLoss, self).__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.cls = classes
        self.dim = dim

    def forward(self, pred, target):
        pred = pred.log_softmax(dim=self.dim)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.cls - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))

def plot_learning_curves(history, out_dir):
    """绘制学习曲线"""
    epochs = range(1, len(history['train_loss']) + 1)
    
    # 创建子图
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # 损失曲线
    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', alpha=0.7)
    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', alpha=0.7)
    ax1.set_title('Training and Validation Loss')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 准确率曲线
    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', alpha=0.7)
    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', alpha=0.7)
    ax2.set_title('Training and Validation Accuracy')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, 'learning_curves.png'), dpi=300, bbox_inches='tight')
    plt.close()

def plot_confusion_matrix(y_true, y_pred, out_dir, epoch=None):
    """绘制混淆矩阵"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Left < Right (0)', 'Left > Right (1)'],
                yticklabels=['Left < Right (0)', 'Left > Right (1)'])
    
    title = 'Confusion Matrix'
    if epoch is not None:
        title += f' (Epoch {epoch})'
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    
    filename = 'confusion_matrix.png'
    if epoch is not None:
        filename = f'confusion_matrix_epoch_{epoch}.png'
    
    plt.savefig(os.path.join(out_dir, filename), dpi=300, bbox_inches='tight')
    plt.close()

def plot_misclassified_samples(loader, model, device, out_dir, num_samples=20):
    """绘制误分类样本"""
    model.eval()
    misclassified = []
    
    with torch.no_grad():
        for xa, xb, y in loader:
            xa = xa.to(device).float()
            xb = xb.to(device).float()
            y = y.to(device).float()
            
            logit = model(xa, xb)
            if isinstance(logit, tuple):
                logit = logit[0]
                
            prob = torch.sigmoid(logit)
            pred = (prob >= 0.5).long()
            
            # 找出误分类的样本
            wrong_mask = (pred.flatten() != y.long()).cpu().numpy()
            wrong_indices = np.where(wrong_mask)[0]
            
            for idx in wrong_indices:
                if len(misclassified) >= num_samples:
                    break
                misclassified.append({
                    'xa': xa[idx].cpu().numpy(),
                    'xb': xb[idx].cpu().numpy(),
                    'true_label': y[idx].cpu().numpy(),
                    'pred_label': pred[idx].cpu().numpy(),
                    'prob': prob[idx].cpu().numpy()
                })
            
            if len(misclassified) >= num_samples:
                break
    
    # 绘制误分类样本
    if misclassified:
        fig, axes = plt.subplots(4, 5, figsize=(15, 12))
        axes = axes.ravel()
        
        for i, sample in enumerate(misclassified[:20]):
            # 合并左右图像
            combined = np.hstack([sample['xa'].squeeze(), sample['xb'].squeeze()])
            axes[i].imshow(combined, cmap='gray')
            axes[i].set_title(f'True: {sample["true_label"]}, Pred: {sample["pred_label"]}\nProb: {sample["prob"]:.3f}')
            axes[i].axis('off')
        
        plt.suptitle('Misclassified Samples (Left | Right)', fontsize=16)
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, 'misclassified_samples.png'), dpi=300, bbox_inches='tight')
        plt.close()

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, required=True)
    ap.add_argument("--out_dir", type=str, default="./outputs/baseline")
    ap.add_argument("--epochs", type=int, default=15)
    ap.add_argument("--batch_size", type=int, default=256)
    ap.add_argument("--lr", type=float, default=2e-3)
    ap.add_argument("--weight_decay", type=float, default=1e-4)
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--num_workers", type=int, default=2)
    ap.add_argument("--use_amp", action="store_true")
    ap.add_argument("--swap_aug", action="store_true")
    ap.add_argument("--pos_weight", type=float, default=1.0, help="BCE pos_weight.")
    ap.add_argument("--label_smoothing", type=float, default=0.1, help="Label smoothing factor.")
    ap.add_argument("--aux_weight", type=float, default=0.4, help="Weight for auxiliary loss.")
    args = ap.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)
    set_seed(args.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CompareNet(feat_dim=128, scale=1.0, symmetric_aux=True).to(device)
    n_params = int(count_params(model))

    train_path = os.path.join(args.data_dir, "train.npz")
    val_path = os.path.join(args.data_dir, "val.npz")
    train_ds = PairNPZDataset(train_path, is_train=True)
    val_ds = PairNPZDataset(val_path, is_train=False)

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,
                              num_workers=args.num_workers, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False,
                              num_workers=args.num_workers, pin_memory=True)

    optim = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=args.epochs, eta_min=args.lr * 0.1)

    # 损失函数
    pos_weight = torch.tensor([args.pos_weight], device=device)
    bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    
    if args.label_smoothing > 0:
        def bce_with_smoothing(logits, y):
            y_smooth = y.float() * (1.0 - args.label_smoothing) + 0.5 * args.label_smoothing
            return torch.nn.functional.binary_cross_entropy_with_logits(logits, y_smooth)
        criterion_main = bce_with_smoothing
    else:
        criterion_main = bce

    aux_bce = torch.nn.BCEWithLogitsLoss()
    scaler = GradScaler(enabled=args.use_amp)

    # 历史记录
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'val_f1': []
    }

    best = {"acc": 0.0, "f1": 0.0, "epoch": -1}
    patience, bad = 5, 0

    for epoch in range(1, args.epochs + 1):
        # 训练阶段
        model.train()
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
        epoch_train_loss = 0
        correct = 0
        total = 0
        
        for xa, xb, y in pbar:
            xa = xa.to(device).float()
            xb = xb.to(device).float()
            y = y.to(device).float()

            if args.swap_aug:
                xa, xb, y = maybe_swap_batch(xa, xb, y)

            optim.zero_grad(set_to_none=True)
            with autocast(enabled=args.use_amp):
                out = model(xa, xb)
                if isinstance(out, tuple):
                    main_logit, aux_logit = out
                    loss_main = criterion_main(main_logit, y.float())
                    loss_aux = aux_bce(aux_logit, y.float())
                    loss = loss_main + args.aux_weight * loss_aux
                else:
                    loss = criterion_main(out, y.float())
                    
                # 计算训练准确率
                prob = torch.sigmoid(main_logit if isinstance(out, tuple) else out)
                pred = (prob >= 0.5).long()
                correct += (pred.flatten() == y.long()).sum().item()
                total += y.size(0)

            epoch_train_loss += loss.item()
            
            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optim)
            scaler.update()
            
            pbar.set_postfix(loss=float(loss.item()), lr=optim.param_groups[0]["lr"])

        scheduler.step()

        # 计算训练准确率
        train_acc = correct / total
        avg_train_loss = epoch_train_loss / len(train_loader)
        
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_acc)

        # 验证阶段
        val_acc, val_f1, val_loss, y_true, y_pred, y_probs = evaluate(model, val_loader, device, use_aux=True)
        
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)
        
        print(f"[Val] epoch={epoch} acc={val_acc:.4f} f1_macro={val_f1:.4f} loss={val_loss:.4f} (params={n_params})")

        # 保存最佳模型
        if val_acc > best["acc"]:
            best = {"acc": val_acc, "f1": val_f1, "epoch": epoch}
            torch.save(model.state_dict(), os.path.join(args.out_dir, "model.pt"))
            
            # 保存最佳epoch的混淆矩阵
            plot_confusion_matrix(y_true, y_pred, args.out_dir, epoch=epoch)
            
            # 保存最佳epoch的误分类样本
            plot_misclassified_samples(val_loader, model, device, args.out_dir)
            
            with open(os.path.join(args.out_dir, "metrics.json"), "w") as f:
                json.dump({
                    "best_val_acc": val_acc, "best_val_f1": val_f1,
                    "best_epoch": epoch, "params": n_params,
                    "history": history
                }, f, indent=2)
            bad = 0
        else:
            bad += 1
            if bad >= patience:
                print("Early stopping.")
                break

    # 绘制最终的学习曲线
    plot_learning_curves(history, args.out_dir)
    
    # 保存历史数据
    with open(os.path.join(args.out_dir, "training_history.json"), "w") as f:
        json.dump(history, f, indent=2)

    print(f"Best @ epoch {best['epoch']}: acc={best['acc']:.4f}, f1_macro={best['f1']:.4f}, params={n_params}")

if __name__ == "__main__":
    main()